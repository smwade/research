{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import models\n",
    "import itertools\n",
    "import cPickle\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60347 1365\n",
      "RNN raw experiments\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1011 Time elapsed 24 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0926 Time elapsed 47 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1051 Time elapsed 22 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0953 Time elapsed 46 seconds\n",
      "Epoch 2 Seen 91644 samples Avg cost 0.0889 Time elapsed 70 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1066 Time elapsed 43 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0957 Time elapsed 85 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1099 Time elapsed 41 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1002 Time elapsed 82 seconds\n",
      "Epoch 2 Seen 91644 samples Avg cost 0.0964 Time elapsed 125 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1049 Time elapsed 98 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0990 Time elapsed 196 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1077 Time elapsed 110 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0964 Time elapsed 218 seconds\n",
      "Epoch 2 Seen 91644 samples Avg cost 0.0919 Time elapsed 365 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1000 Time elapsed 41 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1000 Time elapsed 83 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1051 Time elapsed 42 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0984 Time elapsed 81 seconds\n",
      "Epoch 2 Seen 91644 samples Avg cost 0.0892 Time elapsed 108 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1031 Time elapsed 51 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1072 Time elapsed 97 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1055 Time elapsed 47 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0972 Time elapsed 95 seconds\n",
      "Epoch 2 Seen 91644 samples Avg cost 0.0886 Time elapsed 141 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1041 Time elapsed 107 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1009 Time elapsed 224 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1040 Time elapsed 111 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1006 Time elapsed 237 seconds\n",
      "Epoch 2 Seen 91644 samples Avg cost 0.0868 Time elapsed 370 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1081 Time elapsed 41 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0978 Time elapsed 78 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1056 Time elapsed 36 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0940 Time elapsed 83 seconds\n",
      "Epoch 2 Seen 91644 samples Avg cost 0.0850 Time elapsed 121 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1043 Time elapsed 57 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0933 Time elapsed 130 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.0996 Time elapsed 63 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1007 Time elapsed 132 seconds\n",
      "Epoch 2 Seen 91644 samples Avg cost 0.0925 Time elapsed 202 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1045 Time elapsed 116 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0961 Time elapsed 238 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1013 Time elapsed 178 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0970 Time elapsed 333 seconds\n",
      "Epoch 2 Seen 91644 samples Avg cost 0.0883 Time elapsed 475 seconds\n",
      "RNN CCS experiments\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1030 Time elapsed 21 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.0998 Time elapsed 20 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0996 Time elapsed 41 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1027 Time elapsed 43 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1050 Time elapsed 46 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1003 Time elapsed 92 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1030 Time elapsed 109 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1062 Time elapsed 112 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1070 Time elapsed 222 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1040 Time elapsed 26 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1014 Time elapsed 25 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1043 Time elapsed 50 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1040 Time elapsed 48 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1062 Time elapsed 56 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1003 Time elapsed 109 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1001 Time elapsed 151 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1059 Time elapsed 125 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1019 Time elapsed 255 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.0982 Time elapsed 32 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1047 Time elapsed 33 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1033 Time elapsed 66 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1074 Time elapsed 58 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1023 Time elapsed 61 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.0998 Time elapsed 118 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.0990 Time elapsed 134 seconds\n",
      "Epoch 0 Seen 30548 samples Avg cost 0.1019 Time elapsed 135 seconds\n",
      "Epoch 1 Seen 61096 samples Avg cost 0.1023 Time elapsed 273 seconds\n",
      "RF raw experiments\n",
      "RF CCS experiments\n",
      "LR raw experiments\n",
      "LR CCS experiments\n",
      "LDA experiments\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './results/DM1results.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bcfde6b9fb9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0}{1}{2}results.dat'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_save_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisease_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_followup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './results/DM1results.dat'"
     ]
    }
   ],
   "source": [
    "# initialize global params\n",
    "disease_name = \"DM\"\n",
    "path_to_data = \"./diabetes_survival.csv\"\n",
    "path_to_ccs = \"./ccs_map.txt\"\n",
    "path_to_save_folder = \"./results/\"\n",
    "col = {\"Codes\":\"ObsCodes\"}\n",
    "\n",
    "for n_followup in [1,2,3]: #n_followup is the number of years that we want to predict disease occurrence\n",
    "\n",
    "##################################################\n",
    "################ DATA PREPARATION ################\n",
    "##################################################\n",
    "\n",
    "    # load the data from csv\n",
    "    data = pd.read_csv(path_to_data, index_col=0).fillna(value=\"\")\n",
    "\n",
    "    np.random.seed(1989) # set seed for reproducibility\n",
    "    dis_ids = np.random.permutation(data.loc[(data.Event==1)&\n",
    "                                             (data.Survival>0)&\n",
    "                                             (data.Survival <= n_followup*365)].index)\n",
    "    ctl_ids = np.random.permutation(data.loc[data.Survival>n_followup*365].index)\n",
    "    data[\"Target\"] = -1\n",
    "    data.loc[dis_ids,\"Target\"] = 1\n",
    "    data.loc[ctl_ids,\"Target\"] = 0\n",
    "    print len(ctl_ids),len(dis_ids)\n",
    "\n",
    "    N_dis = len(dis_ids)\n",
    "    N_dis_train = int(.5*N_dis)\n",
    "    N_dis_valid = int(.25*N_dis)\n",
    "\n",
    "    N_ctl = len(ctl_ids)\n",
    "    N_ctl_train = int(.5*N_ctl)\n",
    "    N_ctl_valid = int(.25*N_ctl)\n",
    "    N_ctl_test = N_ctl_valid\n",
    "\n",
    "    data[\"Train\"] = -1\n",
    "    data.loc[dis_ids[:N_dis_train],\"Train\"] = 1 \n",
    "    data.loc[dis_ids[N_dis_train:N_dis_train+N_dis_valid],\"Train\"] = 2 \n",
    "    data.loc[dis_ids[N_dis_train+N_dis_valid:],\"Train\"] = 0\n",
    "    data.loc[ctl_ids[:N_ctl_train],\"Train\"] = 1\n",
    "    data.loc[ctl_ids[N_ctl_train:N_ctl_train+N_ctl_valid],\"Train\"] = 2\n",
    "    data.loc[ctl_ids[N_ctl_train+N_ctl_valid:N_ctl_train+N_ctl_valid+N_ctl_test],\"Train\"] = 0\n",
    "\n",
    "    # create raw tokenized data\n",
    "    dictionary   = models.create_dictionary(data.loc[data.Train==1,col[\"Codes\"]].values)\n",
    "    train_tokens = models.create_tokens(data.loc[data.Train==1,col[\"Codes\"]].values, dictionary)\n",
    "    valid_tokens = models.create_tokens(data.loc[data.Train==2,col[\"Codes\"]].values, dictionary)\n",
    "    test_tokens  = models.create_tokens(data.loc[data.Train==0,col[\"Codes\"]].values, dictionary)\n",
    "\n",
    "    # create ccs tokenized data\n",
    "    ccs_map = models.create_ccs_map(path_to_ccs)\n",
    "    unknown_codes = set(dictionary.keys()).difference(set(ccs_map.keys()))\n",
    "    p = len(set(ccs_map.values()))\n",
    "    for c in unknown_codes:\n",
    "        ccs_map[c] = p\n",
    "    train_ccs_tokens = models.create_tokens(data.loc[data.Train==1,col[\"Codes\"]].values, ccs_map)\n",
    "    valid_ccs_tokens = models.create_tokens(data.loc[data.Train==2,col[\"Codes\"]].values, ccs_map)\n",
    "    test_ccs_tokens  = models.create_tokens(data.loc[data.Train==0,col[\"Codes\"]].values, ccs_map)\n",
    "\n",
    "    # create sparse BOW from raw tokens\n",
    "    sparse_train_data = models.create_sparse_array_from_tokens(train_tokens,len(set(dictionary.values()))+1)\n",
    "    sparse_valid_data = models.create_sparse_array_from_tokens(valid_tokens,len(set(dictionary.values()))+1)\n",
    "    sparse_test_data  = models.create_sparse_array_from_tokens(test_tokens,len(set(dictionary.values()))+1)\n",
    "\n",
    "    # create sparse BOW from ccs tokens  \n",
    "    sparse_ccs_train_data = models.create_sparse_array_from_tokens(train_ccs_tokens,len(set(ccs_map.values()))+1)\n",
    "    sparse_ccs_valid_data = models.create_sparse_array_from_tokens(valid_ccs_tokens,len(set(ccs_map.values()))+1)\n",
    "    sparse_ccs_test_data  = models.create_sparse_array_from_tokens(test_ccs_tokens,len(set(ccs_map.values()))+1)\n",
    "\n",
    "    truth = data.loc[data.Train==0,\"Target\"].values\n",
    "\n",
    "    #################################################\n",
    "    ################## EXPERIMENTS ##################\n",
    "    #################################################\n",
    "\n",
    "    results = {}\n",
    "    results[\"truth\"] = truth\n",
    "\n",
    "    # RNN EXPERIMENT ON RAW TOKENIZED DATA\n",
    "    print \"RNN raw experiments\"\n",
    "    emb_size = [64, 128, 256]\n",
    "    rec_size = [64, 128, 256]\n",
    "    epochs = [2,3]\n",
    "    p = len(set(dictionary.values()))\n",
    "    best_rnn_score = 0\n",
    "    best_rnn_model = None\n",
    "    best_rnn_params = None\n",
    "    for el in itertools.product(*[emb_size, rec_size, epochs]):\n",
    "        params = {\"embed_size\":el[0], \"recurrent_size\":el[1], \"n_epochs\":el[2], \"n_features\":p+1}\n",
    "        score, mod = models.recurrent_nerual_net_model(params, \n",
    "                                                       train_tokens, \n",
    "                                                       list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                       valid_tokens, \n",
    "                                                       list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rnn_score:\n",
    "            best_rnn_score=score\n",
    "            best_rnn_model=mod\n",
    "            best_rnn_params=params\n",
    "    pred_rnn = best_rnn_model.predict(test_tokens)\n",
    "    results[\"rnn_raw\"] = [pred_rnn, best_rnn_params] \n",
    "\n",
    "    # RNN EXPERIMENT ON CCS TOKENIZED DATA\n",
    "    print \"RNN CCS experiments\"\n",
    "    emb_size = [64, 128, 256]\n",
    "    rec_size = [64, 128, 256]\n",
    "    epochs = [1,2]\n",
    "    p = len(set(ccs_map.values()))\n",
    "    best_rnn_score = 0\n",
    "    best_rnn_model = None\n",
    "    best_rnn_params = None\n",
    "    for el in itertools.product(*[emb_size, rec_size, epochs]):\n",
    "        params = {\"embed_size\":el[0], \"recurrent_size\":el[1], \"n_epochs\":el[2], \"n_features\":p+1}\n",
    "        score, mod = models.recurrent_nerual_net_model(params, \n",
    "                                                       train_ccs_tokens, \n",
    "                                                       list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                       valid_ccs_tokens, \n",
    "                                                       list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rnn_score:\n",
    "            best_rnn_score=score\n",
    "            best_rnn_model=mod\n",
    "            best_rnn_params=params\n",
    "    pred_rnn = best_rnn_model.predict(test_ccs_tokens)\n",
    "    results[\"rnn_ccs\"] = [pred_rnn, best_rnn_params]\n",
    "\n",
    "    # RF EXPERIMENT ON RAW TOKENIZED DATA\n",
    "    print \"RF raw experiments\"\n",
    "    best_rfc_score = 0\n",
    "    best_rfc_model = None\n",
    "    best_rfc_params = None\n",
    "    for n in [50, 100, 250, 500]:\n",
    "        params={\"n_trees\":n}\n",
    "        score, mod = models.random_forest_model(params, \n",
    "                                                sparse_train_data,\n",
    "                                                list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                sparse_valid_data, \n",
    "                                                list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rfc_score:\n",
    "            best_rfc_score=score\n",
    "            best_rfc_model=mod\n",
    "            best_rfc_params=params\n",
    "    pred_rfc = best_rfc_model.predict_proba(sparse_test_data)[:,1]\n",
    "    results[\"rfc_raw\"] = [pred_rfc, best_rfc_params]\n",
    "\n",
    "    # RF EXPERIMENT ON CCS TOKENIZED DATA\n",
    "    print \"RF CCS experiments\"\n",
    "    best_rfc_score = 0\n",
    "    best_rfc_model = None\n",
    "    best_rfc_params = None\n",
    "    for n in [50, 100, 250, 500]:\n",
    "        params={\"n_trees\":n}\n",
    "        score, mod = models.random_forest_model(params, \n",
    "                                                sparse_ccs_train_data,\n",
    "                                                list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                sparse_ccs_valid_data, \n",
    "                                                list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rfc_score:\n",
    "            best_rfc_score=score\n",
    "            best_rfc_model=mod\n",
    "            best_rfc_params=params\n",
    "    pred_rfc = best_rfc_model.predict_proba(sparse_ccs_test_data)[:,1]\n",
    "    results[\"rfc_ccs\"] = [pred_rfc, best_rfc_params]\n",
    "\n",
    "    # LR EXPERIMENT ON RAW TOKENIZED DATA\n",
    "    print \"LR raw experiments\"\n",
    "    best_lr_score = 0\n",
    "    best_lr_model = None\n",
    "    best_lr_params = None\n",
    "    for c in [.1, .5, 1.0, 5.0]:\n",
    "        params={\"C\":c}\n",
    "        score, mod = models.logistic_regression_model(params, \n",
    "                                                      sparse_train_data,\n",
    "                                                      list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                      sparse_valid_data, \n",
    "                                                      list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_lr_score:\n",
    "            best_lr_score=score\n",
    "            best_lr_model=mod\n",
    "            best_lr_params=params\n",
    "    pred_lr = best_lr_model.predict_proba(sparse_test_data)[:,1]\n",
    "    results[\"lr_raw\"] = [pred_lr, best_lr_params]\n",
    "\n",
    "    # LR EXPERIMENT ON CCS TOKENIZED DATA\n",
    "    print \"LR CCS experiments\"\n",
    "    best_lr_score = 0\n",
    "    best_lr_model = None\n",
    "    best_lr_params = None\n",
    "    for c in [.1, .5, 1.0, 5.0]:\n",
    "        params={\"C\":c}\n",
    "        score, mod = models.logistic_regression_model(params, \n",
    "                                                      sparse_ccs_train_data,\n",
    "                                                      list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                      sparse_ccs_valid_data, \n",
    "                                                      list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_lr_score:\n",
    "            best_lr_score=score\n",
    "            best_lr_model=mod\n",
    "            best_lr_params=params\n",
    "    pred_lr = best_lr_model.predict_proba(sparse_ccs_test_data)[:,1]\n",
    "    results[\"lr_ccs\"] = [pred_lr, best_lr_params]\n",
    "\n",
    "\n",
    "\n",
    "    # do random forest and LR model selection on LDA dimensionality reduction\n",
    "    print \"LDA experiments\"\n",
    "    best_lr_lda_model = None\n",
    "    best_lr_lda_score = 0\n",
    "    best_lr_gensim_model = None\n",
    "    best_lr_lda_params = None\n",
    "\n",
    "    best_rfc_lda_model = None\n",
    "    best_rfc_lda_score = 0\n",
    "    best_rfc_gensim_model = None\n",
    "    best_rfc_lda_params = None\n",
    "\n",
    "    gensim_d = gensim.corpora.dictionary.Dictionary([c.split(\",\") for c in data.loc[data.Train==1,col[\"Codes\"]]])\n",
    "    gensim_train_corp = [gensim_d.doc2bow(c.split(\",\")) for c in data.loc[data.Train==1, col[\"Codes\"]]]\n",
    "    gensim_valid_corp = [gensim_d.doc2bow(c.split(\",\")) for c in data.loc[data.Train==2, col[\"Codes\"]]]\n",
    "    gensim_test_corp = [gensim_d.doc2bow(c.split(\",\")) for c in data.loc[data.Train==0, col[\"Codes\"]]]\n",
    "    for n_tops in [10,30,50,100]:\n",
    "        gensim_mod = gensim.models.ldamodel.LdaModel(corpus=gensim_train_corp,\n",
    "                                              id2word=gensim_d,\n",
    "                                              num_topics=n_tops, \n",
    "                                              #workers=1,\n",
    "                                              chunksize=10,\n",
    "                                              eval_every=0)\n",
    "\n",
    "        # create low-dimensional vectors\n",
    "        lda_train_vectors = [[t[1] for t in gensim_mod.__getitem__(c, eps=0)] \n",
    "                                       for c in gensim_train_corp]\n",
    "        lda_valid_vectors = [[t[1] for t in gensim_mod.__getitem__(c, eps=0)] \n",
    "                                       for c in gensim_valid_corp]\n",
    "\n",
    "        for n in [50, 100, 500]:\n",
    "            params={\"n_trees\":n, \"n_topics\":n_tops}\n",
    "            score, mod = models.random_forest_model(params, \n",
    "                                                    lda_train_vectors,\n",
    "                                                    list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                    lda_valid_vectors, \n",
    "                                                    list(data.loc[data.Train==2,\"Target\"].values))\n",
    "            if score > best_rfc_lda_score:\n",
    "                best_rfc_lda_score=score\n",
    "                best_rfc_lda_model=mod\n",
    "                best_rfc_lda_params=params\n",
    "                best_rfc_gensim_model = gensim_mod\n",
    "\n",
    "        for c in [.1, .5, 1.0, 5.0]:\n",
    "            params={\"C\":c, \"n_topics\":n_tops}\n",
    "            score, mod = models.logistic_regression_model(params, \n",
    "                                                          lda_train_vectors,\n",
    "                                                          list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                          lda_valid_vectors, \n",
    "                                                          list(data.loc[data.Train==2,\"Target\"].values))\n",
    "            if score > best_lr_lda_score:\n",
    "                best_lr_lda_score=score\n",
    "                best_lr_lda_model=mod\n",
    "                best_lr_lda_params=params\n",
    "                best_lr_gensim_model = gensim_mod\n",
    "\n",
    "    # predict using the best models\n",
    "    lda_test_vectors = [[t[1] for t in best_rfc_gensim_model.__getitem__(c, eps=0)] \n",
    "                        for c in gensim_test_corp]\n",
    "    pred_rfc_lda = best_rfc_lda_model.predict_proba(lda_test_vectors)[:,1]\n",
    "    results[\"rfc_lda\"] = [pred_rfc_lda, best_rfc_lda_params]\n",
    "\n",
    "    lda_test_vectors = [[t[1] for t in best_lr_gensim_model.__getitem__(c, eps=0)] \n",
    "                        for c in gensim_test_corp]\n",
    "    pred_lr_lda = best_lr_lda_model.predict_proba(lda_test_vectors)[:,1]\n",
    "    results[\"lr_lda\"] = [pred_lr_lda, best_lr_lda_params]\n",
    "    \n",
    "    # save results\n",
    "    with open('{0}{1}{2}results.dat'.format(path_to_save_folder,disease_name,n_followup), 'wb') as outfile:\n",
    "        cPickle.dump(results, outfile, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize global params\n",
    "disease_name = \"CKD\"\n",
    "path_to_data = \"./ckd_survival.csv\"\n",
    "path_to_ccs = \"./ccs_map.txt\"\n",
    "path_to_save_folder = \"./results/\"\n",
    "col = {\"Codes\":\"ObsCodes\"}\n",
    "\n",
    "for n_followup in [1,2,3]: #n_followup is the number of years that we want to predict disease occurrence\n",
    "\n",
    "##################################################\n",
    "################ DATA PREPARATION ################\n",
    "##################################################\n",
    "\n",
    "    # load the data from csv\n",
    "    data = pd.read_csv(path_to_data, index_col=0).fillna(value=\"\")\n",
    "\n",
    "    np.random.seed(1989) # set seed for reproducibility\n",
    "    dis_ids = np.random.permutation(data.loc[(data.Event==1)&\n",
    "                                             (data.Survival>0)&\n",
    "                                             (data.Survival <= n_followup*365)].index)\n",
    "    ctl_ids = np.random.permutation(data.loc[data.Survival>n_followup*365].index)\n",
    "    data[\"Target\"] = -1\n",
    "    data.loc[dis_ids,\"Target\"] = 1\n",
    "    data.loc[ctl_ids,\"Target\"] = 0\n",
    "    print len(ctl_ids),len(dis_ids)\n",
    "\n",
    "    N_dis = len(dis_ids)\n",
    "    N_dis_train = int(.5*N_dis)\n",
    "    N_dis_valid = int(.25*N_dis)\n",
    "\n",
    "    N_ctl = len(ctl_ids)\n",
    "    N_ctl_train = int(.5*N_ctl)\n",
    "    N_ctl_valid = int(.25*N_ctl)\n",
    "    N_ctl_test = N_ctl_valid\n",
    "\n",
    "    data[\"Train\"] = -1\n",
    "    data.loc[dis_ids[:N_dis_train],\"Train\"] = 1 \n",
    "    data.loc[dis_ids[N_dis_train:N_dis_train+N_dis_valid],\"Train\"] = 2 \n",
    "    data.loc[dis_ids[N_dis_train+N_dis_valid:],\"Train\"] = 0\n",
    "    data.loc[ctl_ids[:N_ctl_train],\"Train\"] = 1\n",
    "    data.loc[ctl_ids[N_ctl_train:N_ctl_train+N_ctl_valid],\"Train\"] = 2\n",
    "    data.loc[ctl_ids[N_ctl_train+N_ctl_valid:N_ctl_train+N_ctl_valid+N_ctl_test],\"Train\"] = 0\n",
    "\n",
    "    # create raw tokenized data\n",
    "    dictionary   = models.create_dictionary(data.loc[data.Train==1,col[\"Codes\"]].values)\n",
    "    train_tokens = models.create_tokens(data.loc[data.Train==1,col[\"Codes\"]].values, dictionary)\n",
    "    valid_tokens = models.create_tokens(data.loc[data.Train==2,col[\"Codes\"]].values, dictionary)\n",
    "    test_tokens  = models.create_tokens(data.loc[data.Train==0,col[\"Codes\"]].values, dictionary)\n",
    "\n",
    "    # create ccs tokenized data\n",
    "    ccs_map = models.create_ccs_map(path_to_ccs)\n",
    "    unknown_codes = set(dictionary.keys()).difference(set(ccs_map.keys()))\n",
    "    p = len(set(ccs_map.values()))\n",
    "    for c in unknown_codes:\n",
    "        ccs_map[c] = p\n",
    "    train_ccs_tokens = models.create_tokens(data.loc[data.Train==1,col[\"Codes\"]].values, ccs_map)\n",
    "    valid_ccs_tokens = models.create_tokens(data.loc[data.Train==2,col[\"Codes\"]].values, ccs_map)\n",
    "    test_ccs_tokens  = models.create_tokens(data.loc[data.Train==0,col[\"Codes\"]].values, ccs_map)\n",
    "\n",
    "    # create sparse BOW from raw tokens\n",
    "    sparse_train_data = models.create_sparse_array_from_tokens(train_tokens,len(set(dictionary.values()))+1)\n",
    "    sparse_valid_data = models.create_sparse_array_from_tokens(valid_tokens,len(set(dictionary.values()))+1)\n",
    "    sparse_test_data  = models.create_sparse_array_from_tokens(test_tokens,len(set(dictionary.values()))+1)\n",
    "\n",
    "    # create sparse BOW from ccs tokens  \n",
    "    sparse_ccs_train_data = models.create_sparse_array_from_tokens(train_ccs_tokens,len(set(ccs_map.values()))+1)\n",
    "    sparse_ccs_valid_data = models.create_sparse_array_from_tokens(valid_ccs_tokens,len(set(ccs_map.values()))+1)\n",
    "    sparse_ccs_test_data  = models.create_sparse_array_from_tokens(test_ccs_tokens,len(set(ccs_map.values()))+1)\n",
    "\n",
    "    truth = data.loc[data.Train==0,\"Target\"].values\n",
    "\n",
    "    #################################################\n",
    "    ################## EXPERIMENTS ##################\n",
    "    #################################################\n",
    "\n",
    "    results = {}\n",
    "    results[\"truth\"] = truth\n",
    "\n",
    "    # RNN EXPERIMENT ON RAW TOKENIZED DATA\n",
    "    print \"RNN raw experiments\"\n",
    "    emb_size = [64, 128, 256]\n",
    "    rec_size = [64, 128, 256]\n",
    "    epochs = [2,3]\n",
    "    p = len(set(dictionary.values()))\n",
    "    best_rnn_score = 0\n",
    "    best_rnn_model = None\n",
    "    best_rnn_params = None\n",
    "    for el in itertools.product(*[emb_size, rec_size, epochs]):\n",
    "        params = {\"embed_size\":el[0], \"recurrent_size\":el[1], \"n_epochs\":el[2], \"n_features\":p+1}\n",
    "        score, mod = models.recurrent_nerual_net_model(params, \n",
    "                                                       train_tokens, \n",
    "                                                       list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                       valid_tokens, \n",
    "                                                       list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rnn_score:\n",
    "            best_rnn_score=score\n",
    "            best_rnn_model=mod\n",
    "            best_rnn_params=params\n",
    "    pred_rnn = best_rnn_model.predict(test_tokens)\n",
    "    results[\"rnn_raw\"] = [pred_rnn, best_rnn_params] \n",
    "\n",
    "    # RNN EXPERIMENT ON CCS TOKENIZED DATA\n",
    "    print \"RNN CCS experiments\"\n",
    "    emb_size = [64, 128, 256]\n",
    "    rec_size = [64, 128, 256]\n",
    "    epochs = [1,2]\n",
    "    p = len(set(ccs_map.values()))\n",
    "    best_rnn_score = 0\n",
    "    best_rnn_model = None\n",
    "    best_rnn_params = None\n",
    "    for el in itertools.product(*[emb_size, rec_size, epochs]):\n",
    "        params = {\"embed_size\":el[0], \"recurrent_size\":el[1], \"n_epochs\":el[2], \"n_features\":p+1}\n",
    "        score, mod = models.recurrent_nerual_net_model(params, \n",
    "                                                       train_ccs_tokens, \n",
    "                                                       list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                       valid_ccs_tokens, \n",
    "                                                       list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rnn_score:\n",
    "            best_rnn_score=score\n",
    "            best_rnn_model=mod\n",
    "            best_rnn_params=params\n",
    "    pred_rnn = best_rnn_model.predict(test_ccs_tokens)\n",
    "    results[\"rnn_ccs\"] = [pred_rnn, best_rnn_params]\n",
    "\n",
    "    # RF EXPERIMENT ON RAW TOKENIZED DATA\n",
    "    print \"RF raw experiments\"\n",
    "    best_rfc_score = 0\n",
    "    best_rfc_model = None\n",
    "    best_rfc_params = None\n",
    "    for n in [50, 100, 250, 500]:\n",
    "        params={\"n_trees\":n}\n",
    "        score, mod = models.random_forest_model(params, \n",
    "                                                sparse_train_data,\n",
    "                                                list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                sparse_valid_data, \n",
    "                                                list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rfc_score:\n",
    "            best_rfc_score=score\n",
    "            best_rfc_model=mod\n",
    "            best_rfc_params=params\n",
    "    pred_rfc = best_rfc_model.predict_proba(sparse_test_data)[:,1]\n",
    "    results[\"rfc_raw\"] = [pred_rfc, best_rfc_params]\n",
    "\n",
    "    # RF EXPERIMENT ON CCS TOKENIZED DATA\n",
    "    print \"RF CCS experiments\"\n",
    "    best_rfc_score = 0\n",
    "    best_rfc_model = None\n",
    "    best_rfc_params = None\n",
    "    for n in [50, 100, 250, 500]:\n",
    "        params={\"n_trees\":n}\n",
    "        score, mod = models.random_forest_model(params, \n",
    "                                                sparse_ccs_train_data,\n",
    "                                                list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                sparse_ccs_valid_data, \n",
    "                                                list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rfc_score:\n",
    "            best_rfc_score=score\n",
    "            best_rfc_model=mod\n",
    "            best_rfc_params=params\n",
    "    pred_rfc = best_rfc_model.predict_proba(sparse_ccs_test_data)[:,1]\n",
    "    results[\"rfc_ccs\"] = [pred_rfc, best_rfc_params]\n",
    "\n",
    "    # LR EXPERIMENT ON RAW TOKENIZED DATA\n",
    "    print \"LR raw experiments\"\n",
    "    best_lr_score = 0\n",
    "    best_lr_model = None\n",
    "    best_lr_params = None\n",
    "    for c in [.1, .5, 1.0, 5.0]:\n",
    "        params={\"C\":c}\n",
    "        score, mod = models.logistic_regression_model(params, \n",
    "                                                      sparse_train_data,\n",
    "                                                      list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                      sparse_valid_data, \n",
    "                                                      list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_lr_score:\n",
    "            best_lr_score=score\n",
    "            best_lr_model=mod\n",
    "            best_lr_params=params\n",
    "    pred_lr = best_lr_model.predict_proba(sparse_test_data)[:,1]\n",
    "    results[\"lr_raw\"] = [pred_lr, best_lr_params]\n",
    "\n",
    "    # LR EXPERIMENT ON CCS TOKENIZED DATA\n",
    "    print \"LR CCS experiments\"\n",
    "    best_lr_score = 0\n",
    "    best_lr_model = None\n",
    "    best_lr_params = None\n",
    "    for c in [.1, .5, 1.0, 5.0]:\n",
    "        params={\"C\":c}\n",
    "        score, mod = models.logistic_regression_model(params, \n",
    "                                                      sparse_ccs_train_data,\n",
    "                                                      list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                      sparse_ccs_valid_data, \n",
    "                                                      list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_lr_score:\n",
    "            best_lr_score=score\n",
    "            best_lr_model=mod\n",
    "            best_lr_params=params\n",
    "    pred_lr = best_lr_model.predict_proba(sparse_ccs_test_data)[:,1]\n",
    "    results[\"lr_ccs\"] = [pred_lr, best_lr_params]\n",
    "\n",
    "\n",
    "\n",
    "    # do random forest and LR model selection on LDA dimensionality reduction\n",
    "    print \"LDA experiments\"\n",
    "    best_lr_lda_model = None\n",
    "    best_lr_lda_score = 0\n",
    "    best_lr_gensim_model = None\n",
    "    best_lr_lda_params = None\n",
    "\n",
    "    best_rfc_lda_model = None\n",
    "    best_rfc_lda_score = 0\n",
    "    best_rfc_gensim_model = None\n",
    "    best_rfc_lda_params = None\n",
    "\n",
    "    gensim_d = gensim.corpora.dictionary.Dictionary([c.split(\",\") for c in data.loc[data.Train==1,col[\"Codes\"]]])\n",
    "    gensim_train_corp = [gensim_d.doc2bow(c.split(\",\")) for c in data.loc[data.Train==1, col[\"Codes\"]]]\n",
    "    gensim_valid_corp = [gensim_d.doc2bow(c.split(\",\")) for c in data.loc[data.Train==2, col[\"Codes\"]]]\n",
    "    gensim_test_corp = [gensim_d.doc2bow(c.split(\",\")) for c in data.loc[data.Train==0, col[\"Codes\"]]]\n",
    "    for n_tops in [10,30,50,100]:\n",
    "        gensim_mod = gensim.models.ldamodel.LdaModel(corpus=gensim_train_corp,\n",
    "                                              id2word=gensim_d,\n",
    "                                              num_topics=n_tops, \n",
    "                                              #workers=1,\n",
    "                                              chunksize=10,\n",
    "                                              eval_every=0)\n",
    "\n",
    "        # create low-dimensional vectors\n",
    "        lda_train_vectors = [[t[1] for t in gensim_mod.__getitem__(c, eps=0)] \n",
    "                                       for c in gensim_train_corp]\n",
    "        lda_valid_vectors = [[t[1] for t in gensim_mod.__getitem__(c, eps=0)] \n",
    "                                       for c in gensim_valid_corp]\n",
    "\n",
    "        for n in [50, 100, 500]:\n",
    "            params={\"n_trees\":n, \"n_topics\":n_tops}\n",
    "            score, mod = models.random_forest_model(params, \n",
    "                                                    lda_train_vectors,\n",
    "                                                    list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                    lda_valid_vectors, \n",
    "                                                    list(data.loc[data.Train==2,\"Target\"].values))\n",
    "            if score > best_rfc_lda_score:\n",
    "                best_rfc_lda_score=score\n",
    "                best_rfc_lda_model=mod\n",
    "                best_rfc_lda_params=params\n",
    "                best_rfc_gensim_model = gensim_mod\n",
    "\n",
    "        for c in [.1, .5, 1.0, 5.0]:\n",
    "            params={\"C\":c, \"n_topics\":n_tops}\n",
    "            score, mod = models.logistic_regression_model(params, \n",
    "                                                          lda_train_vectors,\n",
    "                                                          list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                          lda_valid_vectors, \n",
    "                                                          list(data.loc[data.Train==2,\"Target\"].values))\n",
    "            if score > best_lr_lda_score:\n",
    "                best_lr_lda_score=score\n",
    "                best_lr_lda_model=mod\n",
    "                best_lr_lda_params=params\n",
    "                best_lr_gensim_model = gensim_mod\n",
    "\n",
    "    # predict using the best models\n",
    "    lda_test_vectors = [[t[1] for t in best_rfc_gensim_model.__getitem__(c, eps=0)] \n",
    "                        for c in gensim_test_corp]\n",
    "    pred_rfc_lda = best_rfc_lda_model.predict_proba(lda_test_vectors)[:,1]\n",
    "    results[\"rfc_lda\"] = [pred_rfc_lda, best_rfc_lda_params]\n",
    "\n",
    "    lda_test_vectors = [[t[1] for t in best_lr_gensim_model.__getitem__(c, eps=0)] \n",
    "                        for c in gensim_test_corp]\n",
    "    pred_lr_lda = best_lr_lda_model.predict_proba(lda_test_vectors)[:,1]\n",
    "    results[\"lr_lda\"] = [pred_lr_lda, best_lr_lda_params]\n",
    "    \n",
    "    # save results\n",
    "    with open('{0}{1}{2}results.dat'.format(path_to_save_folder,disease_name,n_followup), 'wb') as outfile:\n",
    "        cPickle.dump(results, outfile, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
