{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import models\n",
    "import itertools\n",
    "import cPickle\n",
    "#import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './united/ckd_data_diagcodes_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d3b22aee6579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m              for c in united_data.loc[i,\"ObsCodes\"].split(\"|\")]\n\u001b[1;32m      9\u001b[0m     \u001b[0munited_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"NewCodes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdeletions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcodes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0munited_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./united/ckd_data_diagcodes_2.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/formats/format.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1458\u001b[0m             f = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1459\u001b[0m                             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m                             compression=self.compression)\n\u001b[0m\u001b[1;32m   1461\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/common.pyc\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path, mode, encoding, compression, memory_map)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmemory_map\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fileno'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './united/ckd_data_diagcodes_2.csv'"
     ]
    }
   ],
   "source": [
    "#united_data = pd.read_csv(\"united/ckd_data_2.csv\").dropna()\n",
    "united_data = pd.read_csv(\"ckd_survival.csv\").dropna()\n",
    "\n",
    "# just extract diagnosis codes, remove decimal points from united data\n",
    "united_data[\"NewCodes\"] = \"\"\n",
    "for i in united_data.loc[united_data.Survival >= 1].index:\n",
    "    codes = [\",\".join([d for d in c.split(\",\") if d[:2]!=\"p_\" and d[:2]!=\"d_\"]) \n",
    "             for c in united_data.loc[i,\"ObsCodes\"].split(\"|\")]\n",
    "    united_data.loc[i,\"NewCodes\"] = \",\".join([string.translate(h,None,deletions=\".\") for h in codes if h != \"\"])\n",
    "united_data.to_csv(\"./united/ckd_data_diagcodes_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize global params\n",
    "disease_name = \"DM\"\n",
    "dis_names = [\"DM\",\"DM\"]\n",
    "data_pths = [\"./united/dm_data_diagcodes_1.csv\",\"./united/dm_data_diagcodes_2.csv\"]\n",
    "path_to_ccs = \"./ccs_map.txt\"\n",
    "save_pths = [\"./dm_prediction_results/diag_codes/united/one_years_obs/\",\n",
    "             \"./dm_prediction_results/diag_codes/united/two_years_obs/\"]\n",
    "col = {\"Codes\":\"NewCodes\"}\n",
    "\n",
    "for disease_name, path_to_data, path_to_save_folder in zip(dis_names, data_pths, save_pths):\n",
    "##################################################\n",
    "################ DATA PREPARATION ################\n",
    "##################################################\n",
    "    print disease_name\n",
    "    \n",
    "    \n",
    "    \n",
    "    # load the data from csv\n",
    "    data = pd.read_csv(path_to_data, index_col=0).dropna()\n",
    "\n",
    "    n_followup = 2 # specifies the number of years that we want to predict disease occurence\n",
    "\n",
    "    np.random.seed(1989) # set seed for reproducibility\n",
    "    dis_ids = np.random.permutation(data.loc[(data.Event==1)&\n",
    "                                             (data.Survival>0)&\n",
    "                                             (data.Survival <= n_followup*365)].index)\n",
    "    ctl_ids = np.random.permutation(data.loc[data.Survival>n_followup*365].index)\n",
    "    data[\"Target\"] = -1\n",
    "    data.loc[dis_ids,\"Target\"] = 1\n",
    "    data.loc[ctl_ids,\"Target\"] = 0\n",
    "    print len(ctl_ids),len(dis_ids)\n",
    "\n",
    "    N_dis = len(dis_ids)\n",
    "    N_dis_train = int(.5*N_dis)\n",
    "    N_dis_valid = int(.25*N_dis)\n",
    "    # next three lines are for if you want number of control cases to be a fixed factor greater than diseased\n",
    "    factor = 50\n",
    "    N_ctl_train = N_dis_train*factor\n",
    "    N_ctl_valid = N_dis_valid*factor\n",
    "    N_ctl_test = N_dis_valid*factor\n",
    "\n",
    "    N_ctl = len(ctl_ids)\n",
    "    N_ctl_train = int(.5*N_ctl)\n",
    "    N_ctl_valid = int(.25*N_ctl)\n",
    "    N_ctl_test = N_ctl_valid\n",
    "\n",
    "    data[\"Train\"] = -1\n",
    "    data.loc[dis_ids[:N_dis_train],\"Train\"] = 1 \n",
    "    data.loc[dis_ids[N_dis_train:N_dis_train+N_dis_valid],\"Train\"] = 2 \n",
    "    data.loc[dis_ids[N_dis_train+N_dis_valid:],\"Train\"] = 0\n",
    "    data.loc[ctl_ids[:N_ctl_train],\"Train\"] = 1\n",
    "    data.loc[ctl_ids[N_ctl_train:N_ctl_train+N_ctl_valid],\"Train\"] = 2\n",
    "    data.loc[ctl_ids[N_ctl_train+N_ctl_valid:N_ctl_train+N_ctl_valid+N_ctl_test],\"Train\"] = 0\n",
    "\n",
    "    # create raw tokenized data\n",
    "    dictionary   = models.create_dictionary(data.loc[data.Train==1,col[\"Codes\"]].values)\n",
    "    train_tokens = models.create_tokens(data.loc[data.Train==1,col[\"Codes\"]].values, dictionary)\n",
    "    valid_tokens = models.create_tokens(data.loc[data.Train==2,col[\"Codes\"]].values, dictionary)\n",
    "    test_tokens  = models.create_tokens(data.loc[data.Train==0,col[\"Codes\"]].values, dictionary)\n",
    "\n",
    "    # create ccs tokenized data\n",
    "    ccs_map = models.create_ccs_map(path_to_ccs)\n",
    "    unknown_codes = set(dictionary.keys()).difference(set(ccs_map.keys()))\n",
    "    p = len(set(ccs_map.values()))\n",
    "    for c in unknown_codes:\n",
    "        ccs_map[c] = p\n",
    "    train_ccs_tokens = models.create_tokens(data.loc[data.Train==1,col[\"Codes\"]].values, ccs_map)\n",
    "    valid_ccs_tokens = models.create_tokens(data.loc[data.Train==2,col[\"Codes\"]].values, ccs_map)\n",
    "    test_ccs_tokens  = models.create_tokens(data.loc[data.Train==0,col[\"Codes\"]].values, ccs_map)\n",
    "\n",
    "    # create sparse BOW from raw tokens\n",
    "    sparse_train_data = models.create_sparse_array_from_tokens(train_tokens,len(set(dictionary.values()))+1)\n",
    "    sparse_valid_data = models.create_sparse_array_from_tokens(valid_tokens,len(set(dictionary.values()))+1)\n",
    "    sparse_test_data  = models.create_sparse_array_from_tokens(test_tokens,len(set(dictionary.values()))+1)\n",
    "\n",
    "    # create sparse BOW from ccs tokens  \n",
    "    sparse_ccs_train_data = models.create_sparse_array_from_tokens(train_ccs_tokens,len(set(ccs_map.values()))+1)\n",
    "    sparse_ccs_valid_data = models.create_sparse_array_from_tokens(valid_ccs_tokens,len(set(ccs_map.values()))+1)\n",
    "    sparse_ccs_test_data  = models.create_sparse_array_from_tokens(test_ccs_tokens,len(set(ccs_map.values()))+1)\n",
    "\n",
    "\n",
    "    # save stuff for future use\n",
    "    np.savetxt(\"{}true_labels.txt\".format(path_to_save_folder), data.loc[data.Train==0,\"Target\"].values)\n",
    "    np.savetxt(\"{}dis_ids.txt\".format(path_to_save_folder), dis_ids)\n",
    "    np.savetxt(\"{}ctl_ids.txt\".format(path_to_save_folder), ctl_ids)\n",
    "\n",
    "    #################################################\n",
    "    ################## EXPERIMENTS ##################\n",
    "    #################################################\n",
    "\n",
    "    results = {}\n",
    "\n",
    "\n",
    "    # RNN EXPERIMENT ON RAW TOKENIZED DATA\n",
    "    print \"RNN raw experiments\"\n",
    "    emb_size = [128, 256]\n",
    "    rec_size = [128, 256]\n",
    "    epochs = [2]\n",
    "    p = len(set(dictionary.values()))\n",
    "    best_rnn_score = 0\n",
    "    best_rnn_model = None\n",
    "    best_rnn_params = None\n",
    "    for el in itertools.product(*[emb_size, rec_size, epochs]):\n",
    "        params = {\"embed_size\":el[0], \"recurrent_size\":el[1], \"n_epochs\":el[2], \"n_features\":p+1}\n",
    "        score, mod = models.recurrent_nerual_net_model(params, \n",
    "                                                       train_tokens, \n",
    "                                                       list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                       valid_tokens, \n",
    "                                                       list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rnn_score:\n",
    "            best_rnn_score=score\n",
    "            best_rnn_model=mod\n",
    "            best_rnn_params=params\n",
    "    pred_rnn = best_rnn_model.predict(test_tokens)\n",
    "    results[\"rnn_raw\"] = [pred_rnn, best_rnn_params] \n",
    "\n",
    "    # RNN EXPERIMENT ON CCS TOKENIZED DATA\n",
    "    print \"RNN CCS experiments\"\n",
    "    emb_size = [128, 256]\n",
    "    rec_size = [128, 256]\n",
    "    epochs = [2]\n",
    "    p = len(set(ccs_map.values()))\n",
    "    best_rnn_score = 0\n",
    "    best_rnn_model = None\n",
    "    best_rnn_params = None\n",
    "\n",
    "    for el in itertools.product(*[emb_size, rec_size, epochs]):\n",
    "        params = {\"embed_size\":el[0], \"recurrent_size\":el[1], \"n_epochs\":el[2], \"n_features\":p+1}\n",
    "        score, mod = models.recurrent_nerual_net_model(params, \n",
    "                                                       train_ccs_tokens, \n",
    "                                                       list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                       valid_ccs_tokens, \n",
    "                                                       list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rnn_score:\n",
    "            best_rnn_score=score\n",
    "            best_rnn_model=mod\n",
    "            best_rnn_params=params\n",
    "    pred_rnn = best_rnn_model.predict(test_ccs_tokens)\n",
    "    results[\"rnn_ccs\"] = [pred_rnn, best_rnn_params]\n",
    "\n",
    "    # RF EXPERIMENT ON RAW TOKENIZED DATA\n",
    "    print \"RF raw experiments\"\n",
    "    best_rfc_score = 0\n",
    "    best_rfc_model = None\n",
    "    best_rfc_params = None\n",
    "    for n in [50, 100, 250, 500]:\n",
    "        params={\"n_trees\":n}\n",
    "        score, mod = models.random_forest_model(params, \n",
    "                                                sparse_train_data,\n",
    "                                                list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                sparse_valid_data, \n",
    "                                                list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rfc_score:\n",
    "            best_rfc_score=score\n",
    "            best_rfc_model=mod\n",
    "            best_rfc_params=params\n",
    "    pred_rfc = best_rfc_model.predict_proba(sparse_test_data)[:,1]\n",
    "    results[\"rfc_raw\"] = [pred_rfc, best_rfc_params]\n",
    "\n",
    "    # RF EXPERIMENT ON CCS TOKENIZED DATA\n",
    "    print \"RF CCS experiments\"\n",
    "    best_rfc_score = 0\n",
    "    best_rfc_model = None\n",
    "    best_rfc_params = None\n",
    "    for n in [50, 100, 250, 500]:\n",
    "        params={\"n_trees\":n}\n",
    "        score, mod = models.random_forest_model(params, \n",
    "                                                sparse_ccs_train_data,\n",
    "                                                list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                sparse_ccs_valid_data, \n",
    "                                                list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_rfc_score:\n",
    "            best_rfc_score=score\n",
    "            best_rfc_model=mod\n",
    "            best_rfc_params=params\n",
    "    pred_rfc = best_rfc_model.predict_proba(sparse_ccs_test_data)[:,1]\n",
    "    results[\"rfc_ccs\"] = [pred_rfc, best_rfc_params]\n",
    "\n",
    "    # LR EXPERIMENT ON RAW TOKENIZED DATA\n",
    "    print \"LR raw experiments\"\n",
    "    best_lr_score = 0\n",
    "    best_lr_model = None\n",
    "    best_lr_params = None\n",
    "    for c in [.1, .5, 1.0, 5.0]:\n",
    "        params={\"C\":c}\n",
    "        score, mod = models.logistic_regression_model(params, \n",
    "                                                      sparse_train_data,\n",
    "                                                      list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                      sparse_valid_data, \n",
    "                                                      list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_lr_score:\n",
    "            best_lr_score=score\n",
    "            best_lr_model=mod\n",
    "            best_lr_params=params\n",
    "    pred_lr = best_lr_model.predict_proba(sparse_test_data)[:,1]\n",
    "    results[\"lr_raw\"] = [pred_lr, best_lr_params]\n",
    "\n",
    "    # LR EXPERIMENT ON CCS TOKENIZED DATA\n",
    "    print \"LR CCS experiments\"\n",
    "    best_lr_score = 0\n",
    "    best_lr_model = None\n",
    "    best_lr_params = None\n",
    "    for c in [.1, .5, 1.0, 5.0]:\n",
    "        params={\"C\":c}\n",
    "        score, mod = models.logistic_regression_model(params, \n",
    "                                                      sparse_ccs_train_data,\n",
    "                                                      list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                      sparse_ccs_valid_data, \n",
    "                                                      list(data.loc[data.Train==2,\"Target\"].values))\n",
    "        if score > best_lr_score:\n",
    "            best_lr_score=score\n",
    "            best_lr_model=mod\n",
    "            best_lr_params=params\n",
    "    pred_lr = best_lr_model.predict_proba(sparse_ccs_test_data)[:,1]\n",
    "    results[\"lr_ccs\"] = [pred_lr, best_lr_params]\n",
    "\n",
    "    # save results\n",
    "    with open('{}results.dat'.format(path_to_save_folder), 'wb') as outfile:\n",
    "        cPickle.dump(results, outfile, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # CRF EXPERIMENTS\n",
    "    print \"CRF experiments\"\n",
    "    best_crf_score = 0\n",
    "    best_crf_model = None\n",
    "    best_crf_params = None\n",
    "    y_valid_crf = data.loc[data.Train==2,\"Target\"].values\n",
    "    for w in [0,1,3]:\n",
    "        params = {'w':w}\n",
    "        x_train = models.create_crf_tokens(data.loc[data.Train==1,col[\"Codes\"]].values,\n",
    "                                           ccs_map,\n",
    "                                           w)\n",
    "        x_valid = models.create_crf_tokens(data.loc[data.Train==2,col[\"Codes\"]].values,\n",
    "                                           ccs_map,\n",
    "                                           w)\n",
    "        y_train_crf = [[str(lab)]*len(seq) for seq,lab \n",
    "                       in itertools.izip(x_train,data.loc[data[\"Train\"]==1,\"Target\"].values)]\n",
    "        score, mod = models.conditional_random_field_model(params,\n",
    "                                                           x_train,\n",
    "                                                           y_train_crf,\n",
    "                                                           x_valid,\n",
    "                                                           y_valid_crf)\n",
    "        if score > best_crf_score:\n",
    "            best_crf_score = score\n",
    "            best_crf_model = mod\n",
    "            best_crf_params = params  \n",
    "    x_test = models.create_crf_tokens(data.loc[data.Train==0,col[\"Codes\"]].values,\n",
    "                                      ccs_map,\n",
    "                                      best_crf_params['w'])\n",
    "    marg = best_crf_model.predict_marginals(x_test)\n",
    "    pred_crf = [r[-1]['1'] for r in marg]\n",
    "    results[\"crf\"] = [pred_crf, best_crf_params]\n",
    "\n",
    "    # do random forest and LR model selection on LDA dimensionality reduction\n",
    "    print \"LDA experiments\"\n",
    "    best_lr_lda_model = None\n",
    "    best_lr_lda_score = 0\n",
    "    best_lr_gensim_model = None\n",
    "    best_lr_lda_params = None\n",
    "\n",
    "    best_rfc_lda_model = None\n",
    "    best_rfc_lda_score = 0\n",
    "    best_rfc_gensim_model = None\n",
    "    best_rfc_lda_params = None\n",
    "\n",
    "    gensim_d = gensim.corpora.dictionary.Dictionary([c.split(\",\") for c in data.loc[data.Train==1,col[\"Codes\"]]])\n",
    "    gensim_train_corp = [gensim_d.doc2bow(c.split(\",\")) for c in data.loc[data.Train==1, col[\"Codes\"]]]\n",
    "    gensim_valid_corp = [gensim_d.doc2bow(c.split(\",\")) for c in data.loc[data.Train==2, col[\"Codes\"]]]\n",
    "    gensim_test_corp = [gensim_d.doc2bow(c.split(\",\")) for c in data.loc[data.Train==0, col[\"Codes\"]]]\n",
    "    for n_tops in [10,30,100]:\n",
    "        gensim_mod = gensim.models.ldamodel.LdaModel(corpus=gensim_train_corp,\n",
    "                                              id2word=gensim_d,\n",
    "                                              num_topics=n_tops, \n",
    "                                              #workers=1,\n",
    "                                              chunksize=10,\n",
    "                                              eval_every=0)\n",
    "\n",
    "        # create low-dimensional vectors\n",
    "        lda_train_vectors = [[t[1] for t in gensim_mod.__getitem__(c, eps=0)] \n",
    "                                       for c in gensim_train_corp]\n",
    "        lda_valid_vectors = [[t[1] for t in gensim_mod.__getitem__(c, eps=0)] \n",
    "                                       for c in gensim_valid_corp]\n",
    "\n",
    "        for n in [50, 100, 250, 500]:\n",
    "            params={\"n_trees\":n, \"n_topics\":n_tops}\n",
    "            score, mod = models.random_forest_model(params, \n",
    "                                                    lda_train_vectors,\n",
    "                                                    list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                    lda_valid_vectors, \n",
    "                                                    list(data.loc[data.Train==2,\"Target\"].values))\n",
    "            if score > best_rfc_lda_score:\n",
    "                best_rfc_lda_score=score\n",
    "                best_rfc_lda_model=mod\n",
    "                best_rfc_lda_params=params\n",
    "                best_rfc_gensim_model = gensim_mod\n",
    "\n",
    "        for c in [.1, .5, 1.0, 5.0]:\n",
    "            params={\"C\":c, \"n_topics\":n_tops}\n",
    "            score, mod = models.logistic_regression_model(params, \n",
    "                                                          lda_train_vectors,\n",
    "                                                          list(data.loc[data.Train==1,\"Target\"].values),\n",
    "                                                          lda_valid_vectors, \n",
    "                                                          list(data.loc[data.Train==2,\"Target\"].values))\n",
    "            if score > best_lr_lda_score:\n",
    "                best_lr_lda_score=score\n",
    "                best_lr_lda_model=mod\n",
    "                best_lr_lda_params=params\n",
    "                best_lr_gensim_model = gensim_mod\n",
    "\n",
    "    # predict using the best models\n",
    "    lda_test_vectors = [[t[1] for t in best_rfc_gensim_model.__getitem__(c, eps=0)] \n",
    "                        for c in gensim_test_corp]\n",
    "    pred_rfc_lda = best_rfc_lda_model.predict_proba(lda_test_vectors)[:,1]\n",
    "    results[\"rfc_lda\"] = [pred_rfc_lda, best_rfc_lda_params]\n",
    "\n",
    "    lda_test_vectors = [[t[1] for t in best_lr_gensim_model.__getitem__(c, eps=0)] \n",
    "                        for c in gensim_test_corp]\n",
    "    pred_lr_lda = best_lr_lda_model.predict_proba(lda_test_vectors)[:,1]\n",
    "    results[\"lr_lda\"] = [pred_lr_lda, best_lr_lda_params]\n",
    "\n",
    "    # save results\n",
    "    with open('{}results.dat'.format(path_to_save_folder), 'wb') as outfile:\n",
    "        cPickle.dump(results, outfile, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rfc_ccs', 'lr_ccs', 'rnn_raw', 'rfc_raw', 'crf', 'rnn_ccs', 'lr_raw']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
